{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt: \n",
    "\n",
    "Oi, pode implementar o algoritmo KNN, na linguagem python, porém sem usar bibliotecas com o algoritmo já pronto, apenas bibliotecas vetoriais como o numpy.\n",
    "  \n",
    "Resposta:\n",
    "\n",
    "Claro! Aqui está uma implementação simples do algoritmo K-Nearest Neighbors (KNN) em Python utilizando apenas bibliotecas básicas como o NumPy para manipulação de arrays:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições: [1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Armazena os dados de treino.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Prediz os rótulos de dados de teste.\"\"\"\n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        \"\"\"Prediz o rótulo para um único ponto de dados.\"\"\"\n",
    "        # Calcula as distâncias euclidianas de x para todos os pontos de treino\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "\n",
    "        # Pega os índices das k menores distâncias\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        # Pega os rótulos correspondentes a esses k pontos mais próximos\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "        # Retorna o rótulo mais comum (maioria)\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "# Exemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    # Dados de treino (X_train -> características, y_train -> rótulos)\n",
    "    X_train = np.array([[1, 2], [2, 3], [3, 4], [6, 5], [7, 7], [8, 9]])\n",
    "    y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "    # Dados de teste\n",
    "    X_test = np.array([[5, 5], [0, 1]])\n",
    "\n",
    "    # Instanciando o classificador KNN com k=3\n",
    "    knn = KNN(k=3)\n",
    "\n",
    "    # Treinando o modelo\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Fazendo previsões\n",
    "    predictions = knn.predict(X_test)\n",
    "    print(\"Predições:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação:\n",
    "KNN é um algoritmo baseado em instância que armazena todos os dados de treino e, ao prever um novo ponto de dados, ele:\n",
    "Calcula a distância (nesse caso, distância euclidiana) do ponto de teste para todos os pontos de treino.\n",
    "Seleciona os k vizinhos mais próximos (menores distâncias).\n",
    "Faz uma votação majoritária dos rótulos desses vizinhos para determinar a classe de saída.\n",
    "Exemplo de saída:\n",
    "Se executarmos o exemplo acima, as predições podem ser algo como:\n",
    "\n",
    "Predições: [1 0]\n",
    "\n",
    "Esse algoritmo é bastante simples e pode ser ajustado para incluir variações, como diferentes métricas de distância ou técnicas de ponderação para os vizinhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação do algoritmo criado pelo ChatGPT sobre o dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 100.00%\n",
      "Precisão: 100.00%\n",
      "Recall: 100.00%\n",
      "F1-Score: 100.00%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    knn = KNN(k=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    predictions = knn.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='macro')\n",
    "    recall = recall_score(y_test, predictions, average='macro')\n",
    "    f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    print(f\"Acurácia: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precisão: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise crítica do código gerado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Função **fit(self, X_train, y_train)**: Por ser um algoritmo \"lazy\" não constroi um modelo de predição, logo a função fit implementada pelo ChatGPT está de acordo com o que vimos em aula. Ela apenas armazema os conjuntos de treino x e y.\n",
    "\n",
    "- Função **predit(self, X_test)**: O algoritmo implementado pelo ChatGPT está correto, pois ele itera sobre todos as instâncias do conjunto de teste e para cada uma delas ele chama a função _predict(self, x).\n",
    "\n",
    "- Função **_predict(self, x)**: Essa função calcula qual a classe mais frequente, com base no conjunto de treinamento, essa utiliza a distância euclidiana como medida de dissimilaridade e após isso calcula os k-vizinhos mais próximo e retorna o mais frequente entre eles.\n",
    "\n",
    "A parte do código que trata do exemplo de uso foi desconsiderada visto que utilizamos o dataset Iris para testar o algoritmo.\n",
    "\n",
    "Pontos fortes:\n",
    "\n",
    "- Dado o conjunto de testes Iris com poucas instâncias o algoritmo do ChatGPT teve uma acurácia de 100% com um k=3 em uma divisão 70/30 entre dados para treino/teste. Isso mostra que por mais que a implementação tenha sido simples ela conseguiu generalizar bem para esse conjunto de teste.\n",
    "\n",
    "Potencias problemas:\n",
    "\n",
    "- Caso o problema que o KNN implementado pelo ChatGPT esteja tentando resolver for um problema de regressão, ele não irá funcionar, pois sua implementação foi feita apenas pensada para resolver problemas de classificação.\n",
    "\n",
    "Diferenças do algoritmo visto em aula:\n",
    "\n",
    "- O algoritmo do ChatGPT é similar ao visto em aula, que para uma instância não vista começava calculando a dissimilaridade para todas as instâncias de treino, após isso obtinha as k instâncias mais próximas e por fim classificava o dado não visto com o valor da maioria dos k vizinhos.\n",
    "\n",
    "Opções de projeto realizadas pelo ChatGPT:\n",
    "\n",
    "- Utilização do numpy\n",
    "  - A utilização do numpy é entendível visto que ela é amplamente utilizada para fazer diversos tipos de cálculos em python, então foi um acerto ele ter escolhido ela, pois caso não tivesse, poderia ter implementado a fórmula da distância euclidiana errada, por exemplo.\n",
    "\n",
    "Pontos para melhorar:\n",
    "\n",
    "- O algoritmo permite apenas a utilização da distancia euclidiana como medida de dissimilaridade, então um dos pontos que ele podia melhorar é ter outras medidas ou possibilitar o usuário adicionar uma função própria que calcularia a dissimilaridade de duas instâncias.\n",
    "\n",
    "- Caso tivermos muitas instâncias de treino no KNN a inferência pode ser muito demorada, então uma solução que poderia ter sido implementada é a de paralelizar o método de _predict() para cada uma das instâncias de teste ou para um número fixo de threads.\n",
    "\n",
    "- Além disso, o modelo desenvolvido não oferece uma funcionalidade para atribuir pesos variáveis aos “vizinhos” durante o cálculo das predições. Isso contrasta com o algoritmo k-NN do scikit-learn, que permite configurar a ponderação dos pontos vizinhos de três formas distintas: usando pesos uniformes (‘uniform’), onde todos os pontos em cada vizinhança têm o mesmo peso; atribuindo pesos inversamente proporcionais à distância dos pontos (‘distance’), o que confere maior influência aos vizinhos mais próximos; ou através de uma função definida pelo usuário (‘callable’), que calcula os pesos com base em um array de distâncias fornecido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt:\n",
    "\n",
    "Oi, pode implementar o algoritmo Naive Bayes, na linguagem python, porém sem usar bibliotecas com o algoritmo já pronto, apenas bibliotecas vetoriais como o numpy.\n",
    "\n",
    "Resposta:\n",
    "\n",
    "Claro! Vou implementar o algoritmo Naive Bayes em Python sem usar bibliotecas que já têm o algoritmo pronto, utilizando apenas bibliotecas como o numpy para manipulação de vetores e matrizes.\n",
    "\n",
    "Aqui está uma implementação simples do Naive Bayes para classificação binária:\n",
    "\n",
    "Implementação do Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.mean = {}\n",
    "        self.variance = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Separa os dados por classe\n",
    "        self.classes = np.unique(y)\n",
    "        for cls in self.classes:\n",
    "            X_class = X[y == cls]\n",
    "            self.class_priors[cls] = X_class.shape[0] / X.shape[0]  # P(y=cls)\n",
    "            self.mean[cls] = np.mean(X_class, axis=0)  # Média por atributo\n",
    "            self.variance[cls] = np.var(X_class, axis=0)  # Variância por atributo\n",
    "\n",
    "    def _gaussian_pdf(self, x, mean, var):\n",
    "        # Função de densidade de probabilidade Gaussiana\n",
    "        eps = 1e-6  # Para evitar divisão por zero\n",
    "        coeff = 1 / np.sqrt(2 * np.pi * var + eps)\n",
    "        exponent = np.exp(- (x - mean) ** 2 / (2 * var + eps))\n",
    "        return coeff * exponent\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict_instance(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict_instance(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # Calcula a probabilidade posterior para cada classe\n",
    "        for cls in self.classes:\n",
    "            prior = np.log(self.class_priors[cls])  # log(P(y=cls))\n",
    "            conditional = np.sum(np.log(self._gaussian_pdf(x, self.mean[cls], self.variance[cls])))\n",
    "            posterior = prior + conditional\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # Retorna a classe com a maior probabilidade posterior\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Criação de um dataset simples\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.datasets import load_iris\n",
    "\n",
    "    # Carrega o dataset Iris (apenas 2 classes para classificação binária)\n",
    "    data = load_iris()\n",
    "    X = data.data[data.target != 2]  # Mantendo apenas as classes 0 e 1\n",
    "    y = data.target[data.target != 2]\n",
    "\n",
    "    # Divisão em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Instancia o classificador Naive Bayes\n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    # Predição\n",
    "    y_pred = nb.predict(X_test)\n",
    "\n",
    "    # Acurácia\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"Acurácia: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação:\n",
    "Classe NaiveBayes: Implementa o Naive Bayes com suposições de independência entre os atributos (Naive) e que os atributos seguem uma distribuição Gaussiana (ou Normal).\n",
    "fit(X, y): Ajusta o modelo, calculando a média e variância para cada atributo em cada classe, e também as probabilidades a priori das classes.\n",
    "predict(X): Faz a predição para os exemplos passados, calculando a probabilidade posterior para cada classe e selecionando a classe com maior valor.\n",
    "Distribuição Gaussiana: Como assumimos que os dados seguem uma distribuição normal, usamos a função de densidade de probabilidade Gaussiana (implementada em _gaussian_pdf) para calcular as probabilidades condicionais dos atributos.\n",
    "Teste:\n",
    "Este exemplo usa o conjunto de dados Iris (modificado para ser binário) e divide os dados em treino e teste para avaliar o modelo.\n",
    "\n",
    "Espero que isso ajude! Se precisar de mais detalhes ou explicações, é só avisar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste do algoritmo usando a base de dados Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 97.78%\n",
      "Precisão: 97.62%\n",
      "Recall: 97.44%\n",
      "F1-Score: 97.43%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = load_iris()\n",
    "\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    predictions = nb.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='macro')\n",
    "    recall = recall_score(y_test, predictions, average='macro')\n",
    "    f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    print(f\"Acurácia: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precisão: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árvores de decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt:\n",
    "\n",
    "Oi, pode implementar o algoritmo Arvores de decisao, na linguagem python, porém sem usar bibliotecas com o algoritmo já pronto, apenas bibliotecas vetoriais como o numpy.\n",
    "\n",
    "Resposta:\n",
    "\n",
    "Aqui está uma implementação de uma árvore de decisão em Python usando apenas bibliotecas vetoriais, como o numpy. Este código usa o critério de Gini Impurity para dividir os nós:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        m = len(y)\n",
    "        if m == 0:\n",
    "            return 0\n",
    "        class_counts = np.bincount(y)\n",
    "        probabilities = class_counts / m\n",
    "        return 1 - np.sum(probabilities**2)\n",
    "\n",
    "    def _split(self, X, y, feature_idx, threshold):\n",
    "        left_mask = X[:, feature_idx] <= threshold\n",
    "        right_mask = X[:, feature_idx] > threshold\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gini = float('inf')\n",
    "        best_split = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                X_left, y_left, X_right, y_right = self._split(X, y, feature_idx, threshold)\n",
    "\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                m = len(y)\n",
    "                gini_left = self._gini_impurity(y_left)\n",
    "                gini_right = self._gini_impurity(y_right)\n",
    "                weighted_gini = (len(y_left) / m) * gini_left + (len(y_right) / m) * gini_right\n",
    "\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_split = {\n",
    "                        'feature_idx': feature_idx,\n",
    "                        'threshold': threshold,\n",
    "                        'X_left': X_left,\n",
    "                        'y_left': y_left,\n",
    "                        'X_right': X_right,\n",
    "                        'y_right': y_right,\n",
    "                    }\n",
    "        return best_split\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        if n_labels == 1 or n_samples < self.min_samples_split or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'type': 'leaf', 'class': leaf_value}\n",
    "\n",
    "        best_split = self._best_split(X, y)\n",
    "\n",
    "        if best_split is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'type': 'leaf', 'class': leaf_value}\n",
    "\n",
    "        left_subtree = self._build_tree(best_split['X_left'], best_split['y_left'], depth + 1)\n",
    "        right_subtree = self._build_tree(best_split['X_right'], best_split['y_right'], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'type': 'node',\n",
    "            'feature_idx': best_split['feature_idx'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree,\n",
    "        }\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def _predict_single(self, x, tree):\n",
    "        if tree['type'] == 'leaf':\n",
    "            return tree['class']\n",
    "\n",
    "        feature_idx = tree['feature_idx']\n",
    "        threshold = tree['threshold']\n",
    "\n",
    "        if x[feature_idx] <= threshold:\n",
    "            return self._predict_single(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_single(x, tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação:\n",
    "A função fit constrói a árvore a partir dos dados de entrada X e dos rótulos y.\n",
    "O critério de divisão utilizado é a impureza de Gini, calculada na função _gini_impurity.\n",
    "O método _best_split busca a melhor divisão com base na impureza de Gini, tentando várias divisões em cada recurso (feature).\n",
    "A árvore é construída de maneira recursiva na função _build_tree, que também limita a profundidade com max_depth e o número mínimo de amostras com min_samples_split.\n",
    "A função predict faz previsões para novos dados, percorrendo a árvore até encontrar o valor da folha.\n",
    "Este código implementa uma árvore de decisão simples, com a lógica completa de construção e predição baseada no critério de Gini, mas sem usar bibliotecas de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste do algoritmo usando a base de dados Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 95.56%\n",
      "Precisão: 95.56%\n",
      "Recall: 94.87%\n",
      "F1-Score: 94.84%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 11  2]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Dividir os dados em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Criar e treinar o modelo de árvore de decisão\n",
    "    dt = DecisionTree(max_depth=3)\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    # Fazer previsões no conjunto de teste\n",
    "    predictions = dt.predict(X_test)\n",
    "\n",
    "    # Calcular métricas de desempenho\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='macro')\n",
    "    recall = recall_score(y_test, predictions, average='macro')\n",
    "    f1 = f1_score(y_test, predictions, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    # Exibir resultados\n",
    "    print(f\"Acurácia: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precisão: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise crítica do código gerado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Função **_gini_impurity(self, y)**:\n",
    "\n",
    "- Função **fit(self, X, y)**:\n",
    "\n",
    "- Função **_best_split(self, X, y)**:\n",
    "\n",
    "- Função **_split(self, X, y, feature_idx, threshold)**:\n",
    "\n",
    "- Função **_build_tree(self, X, y, depth=0)**:\n",
    "\n",
    "- Função **_most_common_label(self, y)**:\n",
    "\n",
    "- Função **_predict_single(self, x, tree)**:\n",
    "\n",
    "- Função **_predict(self, x)**: Essa função calcula qual a classe mais frequente, com base no conjunto de treinamento, essa utiliza a distância euclidiana como medida de dissimilaridade e após isso calcula os k-vizinhos mais próximo e retorna o mais frequente entre eles.\n",
    "\n",
    "A parte do código que trata do exemplo de uso foi desconsiderada visto que utilizamos o dataset Iris para testar o algoritmo.\n",
    "\n",
    "Pontos fortes:\n",
    "\n",
    "- \n",
    "\n",
    "Potencias problemas:\n",
    "\n",
    "- \n",
    "\n",
    "Diferenças do algoritmo visto em aula:\n",
    "\n",
    "- \n",
    "\n",
    "Opções de projeto realizadas pelo ChatGPT:\n",
    "\n",
    "- Utilização do numpy\n",
    "  - A utilização do numpy é entendível visto que \n",
    "\n",
    "Pontos para melhorar:\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
