{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promt para o ChatGPT\n",
    "\n",
    "Prompt: \n",
    "\n",
    "Oi, pode implementar o algoritmo KNN, na linguagem python, porém sem usar bibliotecas com o algoritmo já pronto, apenas bibliotecas vetoriais como o numpy.\n",
    "  \n",
    "Resposta:\n",
    "\n",
    "Claro! Aqui está uma implementação simples do algoritmo K-Nearest Neighbors (KNN) em Python utilizando apenas bibliotecas básicas como o NumPy para manipulação de arrays:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições: [1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Armazena os dados de treino.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Prediz os rótulos de dados de teste.\"\"\"\n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        \"\"\"Prediz o rótulo para um único ponto de dados.\"\"\"\n",
    "        # Calcula as distâncias euclidianas de x para todos os pontos de treino\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "\n",
    "        # Pega os índices das k menores distâncias\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        # Pega os rótulos correspondentes a esses k pontos mais próximos\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "        # Retorna o rótulo mais comum (maioria)\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "# Exemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    # Dados de treino (X_train -> características, y_train -> rótulos)\n",
    "    X_train = np.array([[1, 2], [2, 3], [3, 4], [6, 5], [7, 7], [8, 9]])\n",
    "    y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "    # Dados de teste\n",
    "    X_test = np.array([[5, 5], [0, 1]])\n",
    "\n",
    "    # Instanciando o classificador KNN com k=3\n",
    "    knn = KNN(k=3)\n",
    "\n",
    "    # Treinando o modelo\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Fazendo previsões\n",
    "    predictions = knn.predict(X_test)\n",
    "    print(\"Predições:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação:\n",
    "KNN é um algoritmo baseado em instância que armazena todos os dados de treino e, ao prever um novo ponto de dados, ele:\n",
    "Calcula a distância (nesse caso, distância euclidiana) do ponto de teste para todos os pontos de treino.\n",
    "Seleciona os k vizinhos mais próximos (menores distâncias).\n",
    "Faz uma votação majoritária dos rótulos desses vizinhos para determinar a classe de saída.\n",
    "Exemplo de saída:\n",
    "Se executarmos o exemplo acima, as predições podem ser algo como:\n",
    "\n",
    "Predições: [1 0]\n",
    "\n",
    "Esse algoritmo é bastante simples e pode ser ajustado para incluir variações, como diferentes métricas de distância ou técnicas de ponderação para os vizinhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação do algoritmo criado pelo ChatGPT sobre o dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 3:\n",
      "Acurácia: 100.00%\n",
      "Precisão: 100.00%\n",
      "Recall: 100.00%\n",
      "F1-Score: 100.00%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "\n",
      "K = 7:\n",
      "Acurácia: 100.00%\n",
      "Precisão: 100.00%\n",
      "Recall: 100.00%\n",
      "F1-Score: 100.00%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "\n",
      "K = 51:\n",
      "Acurácia: 95.56%\n",
      "Precisão: 95.56%\n",
      "Recall: 94.87%\n",
      "F1-Score: 94.84%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  2 11]]\n",
      "\n",
      "K = 105:\n",
      "Acurácia: 57.78%\n",
      "Precisão: 46.88%\n",
      "Recall: 66.67%\n",
      "F1-Score: 52.59%\n",
      "Matriz de Confusão:\n",
      "[[ 0 19  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Utilizado pois no usecase do KNN com 105 o método usado para calcular a precisão estava gerando um warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_test(k):\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    knn = KNN(k)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    predictions = knn.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='macro')\n",
    "    recall = recall_score(y_test, predictions, average='macro')\n",
    "    f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    print(f\"Acurácia: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precisão: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"K = 3:\")\n",
    "    create_test(3)\n",
    "    print(\"\\nK = 7:\")\n",
    "    create_test(5)\n",
    "    print(\"\\nK = 51:\")\n",
    "    create_test(51)\n",
    "    print(\"\\nK = 105:\") # 105 é o número de instâncias usadas para treino\n",
    "    create_test(105)  # underfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise crítica do código gerado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise sobre as funções geradas:\n",
    "\n",
    "- Função **fit(self, X_train, y_train)**: Por ser um algoritmo \"lazy\" não constroi um modelo de predição, logo a função fit implementada pelo ChatGPT está de acordo com o que vimos em aula. Ela apenas armazema os conjuntos de treino x e y.\n",
    "\n",
    "- Função **predit(self, X_test)**: O algoritmo implementado pelo ChatGPT está correto, pois ele itera sobre todos as instâncias do conjunto de teste e para cada uma delas ele chama a função _predict(self, x).\n",
    "\n",
    "- Função **_predict(self, x)**: Essa função calcula qual a classe mais frequente, com base no conjunto de treinamento, essa utiliza a distância euclidiana como medida de dissimilaridade e após isso calcula os k-vizinhos mais próximo e retorna o mais frequente entre eles.\n",
    "\n",
    "A parte do código que trata do exemplo de uso foi desconsiderada visto que utilizamos o dataset Iris para testar o algoritmo.\n",
    "\n",
    "Pontos fortes:\n",
    "\n",
    "- Dado o dataset Iris, os testes obtiveram uma boa performance conseguindo obter 100% de acurácia para os valores de k=3 e k=7, tendo uma divisão 70/30 entre dados para treino/teste. Isso mostra que por mais que a implementação tenha sido simples ela conseguiu generalizar bem para esse conjunto de teste.\n",
    "\n",
    "Potencias problemas:\n",
    "\n",
    "- Caso o problema que o KNN implementado pelo ChatGPT esteja tentando resolver for um problema de regressão, ele não irá funcionar, pois sua implementação foi feita apenas pensada para resolver problemas de classificação.\n",
    "\n",
    "- Funciona apenas para datasets onde as features não sejam categóricas, visto que o algoritmo não faz nenhuma adaptação caso as features sejam categóricas, o que pode ocasionar em um erro.\n",
    "\n",
    "Diferenças do algoritmo visto em aula:\n",
    "\n",
    "- O algoritmo do ChatGPT é similar ao visto em aula, que para uma instância não vista começava calculando a dissimilaridade para todas as instâncias de treino, após isso obtinha as k instâncias mais próximas e por fim classificava o dado não visto com o valor da maioria dos k vizinhos.\n",
    "\n",
    "Opções de projeto realizadas pelo ChatGPT:\n",
    "\n",
    "- Utilização do numpy\n",
    "  - A utilização do numpy é entendível visto que ela é amplamente utilizada para fazer diversos tipos de cálculos em python, então foi um acerto ele ter escolhido ela, pois caso não tivesse, poderia ter implementado a fórmula da distância euclidiana errada, por exemplo.\n",
    "\n",
    "Pontos para melhorar:\n",
    "\n",
    "- O algoritmo permite apenas a utilização da distancia euclidiana como medida de dissimilaridade, então um dos pontos que ele podia melhorar é ter outras medidas ou possibilitar o usuário adicionar uma função própria que calcularia a dissimilaridade de duas instâncias.\n",
    "\n",
    "- Caso tivermos muitas instâncias de treino no KNN a inferência pode ser muito demorada, então uma solução que poderia ter sido implementada é a de paralelizar o método de _predict() para cada uma das instâncias de teste ou para um número fixo de threads.\n",
    "\n",
    "- Além disso, o modelo desenvolvido não oferece uma funcionalidade para atribuir pesos variáveis aos “vizinhos” durante o cálculo das predições. Isso contrasta com o algoritmo k-NN do scikit-learn, que permite configurar a ponderação dos pontos vizinhos de três formas distintas: usando pesos uniformes (‘uniform’), onde todos os pontos em cada vizinhança têm o mesmo peso; atribuindo pesos inversamente proporcionais à distância dos pontos (‘distance’), o que confere maior influência aos vizinhos mais próximos; ou através de uma função definida pelo usuário (‘callable’), que calcula os pesos com base em um array de distâncias fornecido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promt para o ChatGPT\n",
    "\n",
    "Prompt:\n",
    "\n",
    "Oi, pode implementar o algoritmo Naive Bayes, na linguagem python, porém sem usar bibliotecas com o algoritmo já pronto, apenas bibliotecas vetoriais como o numpy.\n",
    "\n",
    "Resposta:\n",
    "\n",
    "Claro! Vou implementar o algoritmo Naive Bayes em Python sem usar bibliotecas que já têm o algoritmo pronto, utilizando apenas bibliotecas como o numpy para manipulação de vetores e matrizes.\n",
    "\n",
    "Aqui está uma implementação simples do Naive Bayes para classificação binária:\n",
    "\n",
    "Implementação do Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.mean = {}\n",
    "        self.variance = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Separa os dados por classe\n",
    "        self.classes = np.unique(y)\n",
    "        for cls in self.classes:\n",
    "            X_class = X[y == cls]\n",
    "            self.class_priors[cls] = X_class.shape[0] / X.shape[0]  # P(y=cls)\n",
    "            self.mean[cls] = np.mean(X_class, axis=0)  # Média por atributo\n",
    "            self.variance[cls] = np.var(X_class, axis=0)  # Variância por atributo\n",
    "\n",
    "    def _gaussian_pdf(self, x, mean, var):\n",
    "        # Função de densidade de probabilidade Gaussiana\n",
    "        eps = 1e-6  # Para evitar divisão por zero\n",
    "        coeff = 1 / np.sqrt(2 * np.pi * var + eps)\n",
    "        exponent = np.exp(- (x - mean) ** 2 / (2 * var + eps))\n",
    "        return coeff * exponent\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict_instance(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict_instance(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # Calcula a probabilidade posterior para cada classe\n",
    "        for cls in self.classes:\n",
    "            prior = np.log(self.class_priors[cls])  # log(P(y=cls))\n",
    "            conditional = np.sum(np.log(self._gaussian_pdf(x, self.mean[cls], self.variance[cls])))\n",
    "            posterior = prior + conditional\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # Retorna a classe com a maior probabilidade posterior\n",
    "        return self.classes[np.argmax(posteriors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação:\n",
    "Classe NaiveBayes: Implementa o Naive Bayes com suposições de independência entre os atributos (Naive) e que os atributos seguem uma distribuição Gaussiana (ou Normal).\n",
    "fit(X, y): Ajusta o modelo, calculando a média e variância para cada atributo em cada classe, e também as probabilidades a priori das classes.\n",
    "predict(X): Faz a predição para os exemplos passados, calculando a probabilidade posterior para cada classe e selecionando a classe com maior valor.\n",
    "Distribuição Gaussiana: Como assumimos que os dados seguem uma distribuição normal, usamos a função de densidade de probabilidade Gaussiana (implementada em _gaussian_pdf) para calcular as probabilidades condicionais dos atributos.\n",
    "\n",
    "Espero que isso ajude! Se precisar de mais detalhes ou explicações, é só avisar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste do algoritmo usando a base de dados Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 97.78%\n",
      "Precisão: 97.62%\n",
      "Recall: 97.44%\n",
      "F1-Score: 97.43%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iris = datasets.load_iris()\n",
    "\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    predictions = nb.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='macro')\n",
    "    recall = recall_score(y_test, predictions, average='macro')\n",
    "    f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    print(f\"Acurácia: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precisão: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise crítica do código gerado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise sobre as funções geradas:\n",
    "\n",
    "- Função **fit(self, X, y)**: Recebe o conjunto de treino X e um vetor y contendo os rótulos de cada classe do dataset. Em seguida, para cada classe calcula os valores que serão utilizados para realizar predições: a média, variância e probabilidade e a priori de um elemento pertencer a esta classe.\n",
    "  \n",
    "- Função **_gaussian_pdf(self, x, mean, var)**: Assume que os dados do conjunto seguem uma distribuição normal e utiliza a fórmula da função densidade de probabilidade Gaussiana para calcular as probabilidades condicionais.\n",
    "  \n",
    "- Função **predict(self, X)**: Função para realizar predições para novos exemplos. Passa por cada dado armazenado no conjunto de teste e utiliza uma função auxiliar para prever a classe.\n",
    "  \n",
    "- Função **_predict_instance(self, x)**: Calcula o log da probabilidade a priori e a prioridade condicional para todos os atributos. A soma do log da prioridade a priori e das probabilidades condicionais resulta na probabilidade a posteriori, e ele utiliza isso para decidir qual classe predizer.\n",
    "\n",
    "Pontos fortes:\n",
    "\n",
    "- A utilização de logaritmo para previnir possíveis problemas numéricos ao trabalhar com números muito pequenos.\n",
    "\n",
    "- É uma implementação fácil de entender o que está sendo feito em cada etapa do código.\n",
    "\n",
    "Potenciais problemas:\n",
    "\n",
    "- Por mais que a utilização de logaritmo ajude a evitar problemas de underflow ela aumenta a complexidade do código, adicionando cálculos a mais para realizar predições.\n",
    "\n",
    "- Essa implementação só está otimizada para problemas com duas classes.\n",
    "\n",
    "- Pela simplificação utilizada através da suposição de distribuição Gaussiana essa implementação não consegue lidar com variáveis categóricas.\n",
    "\n",
    "- Em casos onde a suposição que os dados seguem uma distribuição Gaussiana não for válida, as predições podem apresentar uma precisão menor do que o esperado.\n",
    "\n",
    "Diferenças do algoritmo visto em aula:\n",
    "\n",
    "- A utilização do logaritmo ao invés de multiplicar diretamente as probabilidades.\n",
    "\n",
    "- A não utilização da evidência no calculo da posterior, porém a mesma pode ser desprezada, como visto em aula.\n",
    "\n",
    "- A utilização da distribuição normal para calcular a probabilidade condicional.\n",
    "\n",
    "Opções de projeto realizadas pelo ChatGPT:\n",
    "\n",
    "- Calcular o log das probabilidades a priori de cada instância: De acordo com o próprio chatGPT, esta escolha foi feita para facilitar o cálculo de probabilidades e evitar problemas numéricos. Isso se dá, pois no teorema de Bayes, várias probabilidades devem ser multiplicadas, podendo gerar valores muito pequenos e, por vezes, ocasionando em underflow. Os logaritmos são utilizados para trabalhar com números mais manejáveis.\n",
    "\n",
    "- Utilizar a distribuição Gaussiana: essa escolha foi feita pela simplicidade e eficiência que apresenta para lidar com atributos contínuos. Assume uma distribuição normal para os dados o que possibilita a utilização de uma fórmula fácil de calcular.\n",
    "\n",
    "Pontos para melhorar:\n",
    "\n",
    "- Adicionar suporte para variáveis categóricas.\n",
    "\n",
    "- Suporte para classificação de mais de duas classes.\n",
    "\n",
    "- Validação e normalização de dados\n",
    "\n",
    "- Tratar com mais cuidado uma possível divisão por zero.\n",
    "\n",
    "- Calcular a probabilidade condicional sem a suposição de distribuição normal, ocasionando em melhores predições para problemas onde esta suposição não é válida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árvores de decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promt para o ChatGPT\n",
    "\n",
    "Prompt:\n",
    "\n",
    "Oi, pode implementar o algoritmo Arvores de decisao, na linguagem python, porém sem usar bibliotecas com o algoritmo já pronto, apenas bibliotecas vetoriais como o numpy.\n",
    "\n",
    "Resposta:\n",
    "\n",
    "Aqui está uma implementação de uma árvore de decisão em Python usando apenas bibliotecas vetoriais, como o numpy. Este código usa o critério de Gini Impurity para dividir os nós:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        m = len(y)\n",
    "        if m == 0:\n",
    "            return 0\n",
    "        class_counts = np.bincount(y)\n",
    "        probabilities = class_counts / m\n",
    "        return 1 - np.sum(probabilities**2)\n",
    "\n",
    "    def _split(self, X, y, feature_idx, threshold):\n",
    "        left_mask = X[:, feature_idx] <= threshold\n",
    "        right_mask = X[:, feature_idx] > threshold\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gini = float('inf')\n",
    "        best_split = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                X_left, y_left, X_right, y_right = self._split(X, y, feature_idx, threshold)\n",
    "\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                m = len(y)\n",
    "                gini_left = self._gini_impurity(y_left)\n",
    "                gini_right = self._gini_impurity(y_right)\n",
    "                weighted_gini = (len(y_left) / m) * gini_left + (len(y_right) / m) * gini_right\n",
    "\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_split = {\n",
    "                        'feature_idx': feature_idx,\n",
    "                        'threshold': threshold,\n",
    "                        'X_left': X_left,\n",
    "                        'y_left': y_left,\n",
    "                        'X_right': X_right,\n",
    "                        'y_right': y_right,\n",
    "                    }\n",
    "        return best_split\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        if n_labels == 1 or n_samples < self.min_samples_split or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'type': 'leaf', 'class': leaf_value}\n",
    "\n",
    "        best_split = self._best_split(X, y)\n",
    "\n",
    "        if best_split is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'type': 'leaf', 'class': leaf_value}\n",
    "\n",
    "        left_subtree = self._build_tree(best_split['X_left'], best_split['y_left'], depth + 1)\n",
    "        right_subtree = self._build_tree(best_split['X_right'], best_split['y_right'], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'type': 'node',\n",
    "            'feature_idx': best_split['feature_idx'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree,\n",
    "        }\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def _predict_single(self, x, tree):\n",
    "        if tree['type'] == 'leaf':\n",
    "            return tree['class']\n",
    "\n",
    "        feature_idx = tree['feature_idx']\n",
    "        threshold = tree['threshold']\n",
    "\n",
    "        if x[feature_idx] <= threshold:\n",
    "            return self._predict_single(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_single(x, tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação: A função fit constrói a árvore a partir dos dados de entrada X e dos rótulos y. O critério de divisão utilizado é a impureza de Gini, calculada na função _gini_impurity. O método _best_split busca a melhor divisão com base na impureza de Gini, tentando várias divisões em cada recurso (feature). A árvore é construída de maneira recursiva na função _build_tree, que também limita a profundidade com max_depth e o número mínimo de amostras com min_samples_split. A função predict faz previsões para novos dados, percorrendo a árvore até encontrar o valor da folha. Este código implementa uma árvore de decisão simples, com a lógica completa de construção e predição baseada no critério de Gini, mas sem usar bibliotecas de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste do algoritmo usando a base de dados Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 97.78%\n",
      "Precisão: 97.62%\n",
      "Recall: 97.44%\n",
      "F1-Score: 97.43%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Dividir os dados em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Criar e treinar o modelo de árvore de decisão\n",
    "    dt = DecisionTree(max_depth=2)\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    # Fazer previsões no conjunto de teste\n",
    "    predictions = dt.predict(X_test)\n",
    "\n",
    "    # Calcular métricas de desempenho\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='macro')\n",
    "    recall = recall_score(y_test, predictions, average='macro')\n",
    "    f1 = f1_score(y_test, predictions, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    # Exibir resultados\n",
    "    print(f\"Acurácia: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precisão: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise crítica do código gerado\n",
    "\n",
    "- Função **fit(self, X, y)**: Função que faz o fit do conjunto de treino, essa salva a árvore criada pela função *_build_tree(X, y)*.\n",
    "\n",
    "- Função **_build_tree(self, X, y, depth=0)**: Função recursiva que cria a árvore de decisão e manda calcular a melhor forma de dividir os ramos da árvore usando a função *best_split(X, y)*, caso não tenha nenhuma melhor forma de dividir a árvore cria uma folha e atribui o valor a ela da classe mais comum do conjunto y. Também pode ter condições de paradas que foram definidas no construtor, então caso o conjunto de dados não tenha atingido um mínimo de amostras, a árvore esteja atingido a profundidade máxima permitida ou se só tiver 1 classe no conjunto y, é retornado uma folha com o valor mais comum do conjunto y. Após isso manda dividir recursivamente suas subárvores da esquerda e direita.\n",
    "\n",
    "- Função **_best_split(self, X, y)**: Função que itera sobre todas as features, definindo possíveis thresholds para cada uma, após isso para cada threshold é calculado o grau de impureza da divisão binária do ramo, assim, é retornado a melhor divisão possível usando o índice gini de impureza para uma divisão. O gini split é calculado da mesma forma que foi vista em aula.\n",
    "\n",
    "- Função **_gini_impurity(self, y)**: Função que faz o cálculo do Gini, faz da mesma forma que vimos em aula.\n",
    "\n",
    "- Função **_split(self, X, y, feature_idx, threshold)**: Função que faz o split da feature dado um threashold\n",
    "\n",
    "- Função **_most_common_label(self, y)**: Função que retorna a classe mais comum em uma lista y de classes.\n",
    "\n",
    "- Função **predict(self, x)**: Função que faz a predição de novas insâncias com base no conjunto de treino, para isso, chama a função *_predict_single()* para cada uma das instâncias de x.\n",
    "\n",
    "- Função **_predict_single(self, x, tree)**: Função recursiva que infere qual vai ser a classe de uma nova instância dado o conjunto de treino. Para isso percore a árvore se baseando nos threasholds e quando chega em uma folha, retorna o valor da classe que está dentro dela.\n",
    "\n",
    "Pontos fortes:\n",
    "\n",
    "- Faz o cálculo de impureza usando gini para descobrir qual é a melhor divisão possível, menos impura, da árvore. Para fazer isso o ChatGPT utilizou a medida de impureza do índice gini para uma divisão, em aula vimos isso como GINI split, e a fórmula que o ChatGPT usou para fazer isso foi identica a vista em aula.\n",
    "\n",
    "- Permite o usuário definir alguns critérios de parada da árvore, como a profundida máxima dela e o número mínimo de instâncias na divisão. Isso faz com que o usuário possa controlar melhor o tamanho da árvore.\n",
    "\n",
    "Potencias problemas:\n",
    "\n",
    "- Não aplica nenhum tipo de pós-poda, o que dependendo dos dados de treino poderia solucionar um problema de overfitting.\n",
    "\n",
    "- O algoritmo criado pelo ChatGPT cria apenas árvores binárias, o que restringe bastante a possibilidade de árvores que podemos criar para determinados dados.\n",
    "\n",
    "- Não funciona caso o dataset tiver features categóricas, visto que o algoritmo tenta dividir os dados, como maiores e menores, a partir de um threashold, para features categóricas isso não funcionaria e ocasionaria em um erro.\n",
    "\n",
    "Diferenças do algoritmo visto em aula:\n",
    "\n",
    "- No algoritmo visto em aula de indução Top-down foi visto que é possível dividir um nó em multiplos caminhos, tanto para atributos ordinais quando para atributos categóricos, porém no algoritmo feito pelo ChatGPT ele possibilita apenas a divisão binária e que os atributos sejam ordinais.\n",
    "\n",
    "- Outra difereça para o algoritmo visto em aula é a falta de pós-poda, no algoritmo feito pelo ChatGPT após finalizarmos a criação da árvore de maneira top-down, ele acaba.\n",
    "\n",
    "- Em aula foi visto algumas formas de adaptar árvores de decisão para resolver problemas de regressão, como árvores de regressão e árvores modelo, o algoritmo feito pelo ChatGPT não possui nenhum tipo de adaptação ou de opção voltada a problemas de regressão.\n",
    "\n",
    "Opções de projeto realizadas pelo ChatGPT:\n",
    "\n",
    "- Utilização do numpy\n",
    "  - A utilização do numpy é entendível visto que ela é amplamente utilizada para fazer diversos tipos de cálculos em python, então foi um acerto ele ter escolhido ela, pois caso não tivesse, poderia ter implementado algumas operações mais simples de forma errada.\n",
    "  \n",
    "Pontos para melhorar:\n",
    "\n",
    "- O principal ponto que poderia ser melhorado no algoritmo do ChatGPT é a possibilidade de termos divisões que não sejam binárias, pois isso acaba limitando muito o poder da árvore gerada.\n",
    "  \n",
    "- Outro ponto a melhorar no algoritmo feito pelo ChatGPT é da possibilidade de termos features categóricas no dataset, visto que no método *_split()* ele utiliza operandos de > e < para fazer as divisões de nós filhos, o que impossibilita esse tipo de feature, visto que seus valores não possuem qualquer tipo de ordem entre sí."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
