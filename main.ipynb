{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promt para o ChatGPT\n",
    "\n",
    "Prompt: \n",
    "\n",
    "Oi, pode implementar o algoritmo KNN, na linguagem python, porém sem usar bibliotecas com o algoritmo já pronto, apenas bibliotecas vetoriais como o numpy.\n",
    "  \n",
    "Resposta:\n",
    "\n",
    "Claro! Aqui está uma implementação simples do algoritmo K-Nearest Neighbors (KNN) em Python utilizando apenas bibliotecas básicas como o NumPy para manipulação de arrays:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições: [1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Armazena os dados de treino.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Prediz os rótulos de dados de teste.\"\"\"\n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        \"\"\"Prediz o rótulo para um único ponto de dados.\"\"\"\n",
    "        # Calcula as distâncias euclidianas de x para todos os pontos de treino\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "        \n",
    "        # Pega os índices das k menores distâncias\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # Pega os rótulos correspondentes a esses k pontos mais próximos\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        \n",
    "        # Retorna o rótulo mais comum (maioria)\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "# Exemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    # Dados de treino (X_train -> características, y_train -> rótulos)\n",
    "    X_train = np.array([[1, 2], [2, 3], [3, 4], [6, 5], [7, 7], [8, 9]])\n",
    "    y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "    \n",
    "    # Dados de teste\n",
    "    X_test = np.array([[5, 5], [0, 1]])\n",
    "\n",
    "    # Instanciando o classificador KNN com k=3\n",
    "    knn = KNN(k=3)\n",
    "    \n",
    "    # Treinando o modelo\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Fazendo previsões\n",
    "    predictions = knn.predict(X_test)\n",
    "    print(\"Predições:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação:\n",
    "KNN é um algoritmo baseado em instância que armazena todos os dados de treino e, ao prever um novo ponto de dados, ele:\n",
    "Calcula a distância (nesse caso, distância euclidiana) do ponto de teste para todos os pontos de treino.\n",
    "Seleciona os k vizinhos mais próximos (menores distâncias).\n",
    "Faz uma votação majoritária dos rótulos desses vizinhos para determinar a classe de saída.\n",
    "Exemplo de saída:\n",
    "Se executarmos o exemplo acima, as predições podem ser algo como:\n",
    "\n",
    "Predições: [1 0]\n",
    "\n",
    "Esse algoritmo é bastante simples e pode ser ajustado para incluir variações, como diferentes métricas de distância ou técnicas de ponderação para os vizinhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação do algoritmo criado pelo ChatGPT sobre o dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 3:\n",
      "Acurácia: 100.00%\n",
      "Precisão: 100.00%\n",
      "Recall: 100.00%\n",
      "F1-Score: 100.00%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "\n",
      "K = 7:\n",
      "Acurácia: 100.00%\n",
      "Precisão: 100.00%\n",
      "Recall: 100.00%\n",
      "F1-Score: 100.00%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "\n",
      "K = 51:\n",
      "Acurácia: 95.56%\n",
      "Precisão: 95.56%\n",
      "Recall: 94.87%\n",
      "F1-Score: 94.84%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  2 11]]\n",
      "\n",
      "K = 105:\n",
      "Acurácia: 57.78%\n",
      "Precisão: 46.88%\n",
      "Recall: 66.67%\n",
      "F1-Score: 52.59%\n",
      "Matriz de Confusão:\n",
      "[[ 0 19  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Utilizado pois no usecase do KNN com 105 o método usado para calcular a precisão estava gerando um warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_test(k):\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    knn = KNN(k)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    predictions = knn.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='macro')\n",
    "    recall = recall_score(y_test, predictions, average='macro')\n",
    "    f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    print(f\"Acurácia: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precisão: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"K = 3:\")\n",
    "    create_test(3)\n",
    "    print(\"\\nK = 7:\")\n",
    "    create_test(5)\n",
    "    print(\"\\nK = 51:\")\n",
    "    create_test(51)\n",
    "    print(\"\\nK = 105:\") # 105 é o número de instâncias usadas para treino\n",
    "    create_test(105)  # underfitting\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise crítica do código gerado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise sobre as funções geradas:\n",
    "\n",
    "- Função **fit(self, X_train, y_train)**: Por ser um algoritmo \"lazy\" não constroi um modelo de predição, logo a função fit implementada pelo ChatGPT está de acordo com o que vimos em aula. Ela apenas armazema os conjuntos de treino x e y.\n",
    "\n",
    "- Função **predit(self, X_test)**: O algoritmo implementado pelo ChatGPT está correto, pois ele itera sobre todos as instâncias do conjunto de teste e para cada uma delas ele chama a função _predict(self, x).\n",
    "\n",
    "- Função **_predict(self, x)**: Essa função calcula qual a classe mais frequente, com base no conjunto de treinamento, essa utiliza a distância euclidiana como medida de dissimilaridade e após isso calcula os k-vizinhos mais próximo e retorna o mais frequente entre eles.\n",
    "\n",
    "A parte do código que trata do exemplo de uso foi desconsiderada visto que utilizamos o dataset Iris para testar o algoritmo.\n",
    "\n",
    "Pontos fortes:\n",
    "\n",
    "- Dado o dataset Iris, os testes obtiveram uma boa performance conseguindo obter 100% de acurácia para os valores de k=3 e k=7, tendo uma divisão 70/30 entre dados para treino/teste. Isso mostra que por mais que a implementação tenha sido simples ela conseguiu generalizar bem para esse conjunto de teste.\n",
    "\n",
    "Potencias problemas:\n",
    "\n",
    "- Caso o problema que o KNN implementado pelo ChatGPT esteja tentando resolver for um problema de regressão, ele não irá funcionar, pois sua implementação foi feita apenas pensada para resolver problemas de classificação.\n",
    "\n",
    "Diferenças do algoritmo visto em aula:\n",
    "\n",
    "- O algoritmo do ChatGPT é similar ao visto em aula, que para uma instância não vista começava calculando a dissimilaridade para todas as instâncias de treino, após isso obtinha as k instâncias mais próximas e por fim classificava o dado não visto com o valor da maioria dos k vizinhos.\n",
    "\n",
    "Opções de projeto realizadas pelo ChatGPT:\n",
    "\n",
    "- Utilização do numpy\n",
    "  - A utilização do numpy é entendível visto que ela é amplamente utilizada para fazer diversos tipos de cálculos em python, então foi um acerto ele ter escolhido ela, pois caso não tivesse, poderia ter implementado a fórmula da distância euclidiana errada, por exemplo.\n",
    "\n",
    "Pontos para melhorar:\n",
    "\n",
    "- O algoritmo permite apenas a utilização da distancia euclidiana como medida de dissimilaridade, então um dos pontos que ele podia melhorar é ter outras medidas ou possibilitar o usuário adicionar uma função própria que calcularia a dissimilaridade de duas instâncias.\n",
    "\n",
    "- Caso tivermos muitas instâncias de treino no KNN a inferência pode ser muito demorada, então uma solução que poderia ter sido implementada é a de paralelizar o método de _predict() para cada uma das instâncias de teste ou para um número fixo de threads.\n",
    "\n",
    "- Além disso, o modelo desenvolvido não oferece uma funcionalidade para atribuir pesos variáveis aos “vizinhos” durante o cálculo das predições. Isso contrasta com o algoritmo k-NN do scikit-learn, que permite configurar a ponderação dos pontos vizinhos de três formas distintas: usando pesos uniformes (‘uniform’), onde todos os pontos em cada vizinhança têm o mesmo peso; atribuindo pesos inversamente proporcionais à distância dos pontos (‘distance’), o que confere maior influência aos vizinhos mais próximos; ou através de uma função definida pelo usuário (‘callable’), que calcula os pesos com base em um array de distâncias fornecido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promt para o ChatGPT\n",
    "\n",
    "Prompt:\n",
    "\n",
    "Oi, pode implementar o algoritmo Naive Bayes, na linguagem python, porém sem usar bibliotecas com o algoritmo já pronto, apenas bibliotecas vetoriais como o numpy.\n",
    "\n",
    "Resposta:\n",
    "\n",
    "Claro! Vou implementar o algoritmo Naive Bayes em Python sem usar bibliotecas que já têm o algoritmo pronto, utilizando apenas bibliotecas como o numpy para manipulação de vetores e matrizes.\n",
    "\n",
    "Aqui está uma implementação simples do Naive Bayes para classificação binária:\n",
    "\n",
    "Implementação do Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.mean = {}\n",
    "        self.variance = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Separa os dados por classe\n",
    "        self.classes = np.unique(y)\n",
    "        for cls in self.classes:\n",
    "            X_class = X[y == cls]\n",
    "            self.class_priors[cls] = X_class.shape[0] / X.shape[0]  # P(y=cls)\n",
    "            self.mean[cls] = np.mean(X_class, axis=0)  # Média por atributo\n",
    "            self.variance[cls] = np.var(X_class, axis=0)  # Variância por atributo\n",
    "            \n",
    "    def _gaussian_pdf(self, x, mean, var):\n",
    "        # Função de densidade de probabilidade Gaussiana\n",
    "        eps = 1e-6  # Para evitar divisão por zero\n",
    "        coeff = 1 / np.sqrt(2 * np.pi * var + eps)\n",
    "        exponent = np.exp(- (x - mean) ** 2 / (2 * var + eps))\n",
    "        return coeff * exponent\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict_instance(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict_instance(self, x):\n",
    "        posteriors = []\n",
    "        \n",
    "        # Calcula a probabilidade posterior para cada classe\n",
    "        for cls in self.classes:\n",
    "            prior = np.log(self.class_priors[cls])  # log(P(y=cls))\n",
    "            conditional = np.sum(np.log(self._gaussian_pdf(x, self.mean[cls], self.variance[cls])))\n",
    "            posterior = prior + conditional\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # Retorna a classe com a maior probabilidade posterior\n",
    "        return self.classes[np.argmax(posteriors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação:\n",
    "Classe NaiveBayes: Implementa o Naive Bayes com suposições de independência entre os atributos (Naive) e que os atributos seguem uma distribuição Gaussiana (ou Normal).\n",
    "fit(X, y): Ajusta o modelo, calculando a média e variância para cada atributo em cada classe, e também as probabilidades a priori das classes.\n",
    "predict(X): Faz a predição para os exemplos passados, calculando a probabilidade posterior para cada classe e selecionando a classe com maior valor.\n",
    "Distribuição Gaussiana: Como assumimos que os dados seguem uma distribuição normal, usamos a função de densidade de probabilidade Gaussiana (implementada em _gaussian_pdf) para calcular as probabilidades condicionais dos atributos.\n",
    "\n",
    "Espero que isso ajude! Se precisar de mais detalhes ou explicações, é só avisar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste do algoritmo usando a base de dados Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.1119667 , 0.13658689, 0.03325702, 0.01152966]), 1: array([0.27532505, 0.08724617, 0.23934259, 0.04134405]), 2: array([0.42241052, 0.09630387, 0.28842951, 0.08591673])}\n",
      "{0: array([4.96451613, 3.37741935, 1.46451613, 0.2483871 ]), 1: array([5.86216216, 2.72432432, 4.21081081, 1.3027027 ]), 2: array([6.55945946, 2.98648649, 5.54594595, 2.00540541])}\n",
      "{0: 0.29523809523809524, 1: 0.3523809523809524, 2: 0.3523809523809524}\n",
      "Acurácia: 97.78%\n",
      "Precisão: 97.62%\n",
      "Recall: 97.44%\n",
      "F1-Score: 97.43%\n",
      "Matriz de Confusão:\n",
      "[[19  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iris = datasets.load_iris()    \n",
    "\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    predictions = nb.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='macro')\n",
    "    recall = recall_score(y_test, predictions, average='macro')\n",
    "    f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    print(f\"Acurácia: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precisão: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise crítica do código gerado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise sobre as funções geradas:\n",
    "\n",
    "- Função **fit(self, X, y)**: \n",
    "- Função **_gaussian_pdf(self, x, mean, var)**:\n",
    "- Função **predict(self, X)**:\n",
    "- Função **_predict_instance(self, x)**:\n",
    "\n",
    "Pontos fortes:\n",
    "\n",
    "Potenciais problemas:\n",
    "\n",
    "Diferenças do algoritmo visto em aula:\n",
    "\n",
    "Opções de projeto realizadas pelo ChatGPT:\n",
    "\n",
    "Pontos para melhorar:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
